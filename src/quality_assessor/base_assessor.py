"""
Base Quality Assessor Module
Temel kalite deÄŸerlendirme fonksiyonalitesi
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from abc import ABC, abstractmethod
import logging
from pathlib import Path

# Logger konfigÃ¼rasyonu
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class QualityMetrics:
    """Kalite metriklerini tutan veri sÄ±nÄ±fÄ±"""
    overall_score: float
    image_quality_score: float
    annotation_quality_score: float
    completeness_score: float
    diversity_score: float
    consistency_score: float
    
    # DetaylÄ± metrikler
    total_images: int
    total_annotations: int
    num_classes: int
    class_balance_score: float
    image_resolution_score: float
    annotation_accuracy_score: float
    
    # Ek bilgiler
    issues_found: List[str]
    recommendations: List[str]
    dataset_grade: str  # A, B, C, D
    
    def to_dict(self) -> Dict[str, Any]:
        """Metrikleri dictionary'ye dÃ¶nÃ¼ÅŸtÃ¼r"""
        return {
            'overall_score': self.overall_score,
            'image_quality_score': self.image_quality_score,
            'annotation_quality_score': self.annotation_quality_score,
            'completeness_score': self.completeness_score,
            'diversity_score': self.diversity_score,
            'consistency_score': self.consistency_score,
            'total_images': self.total_images,
            'total_annotations': self.total_annotations,
            'num_classes': self.num_classes,
            'class_balance_score': self.class_balance_score,
            'image_resolution_score': self.image_resolution_score,
            'annotation_accuracy_score': self.annotation_accuracy_score,
            'issues_found': self.issues_found,
            'recommendations': self.recommendations,
            'dataset_grade': self.dataset_grade
        }

class BaseQualityAssessor(ABC):
    """
    Temel kalite deÄŸerlendirici sÄ±nÄ±fÄ±
    TÃ¼m kalite deÄŸerlendirme modÃ¼lleri bu sÄ±nÄ±ftan tÃ¼retilecek
    """
    
    def __init__(self, config: Optional[Dict] = None):
        self.config = config or self._get_default_config()
        self.metrics = None
        self.logger = logging.getLogger(self.__class__.__name__)
        
    def _get_default_config(self) -> Dict:
        """VarsayÄ±lan konfigÃ¼rasyonu dÃ¶ndÃ¼r"""
        return {
            'quality_thresholds': {
                'excellent': 90,
                'good': 75,
                'fair': 60,
                'poor': 0
            },
            'minimum_requirements': {
                'min_images_per_class': 50,
                'min_total_images': 500,
                'min_resolution': 224,
                'max_class_imbalance': 0.8
            },
            'weights': {
                'image_quality': 0.25,
                'annotation_quality': 0.25,
                'completeness': 0.20,
                'diversity': 0.15,
                'consistency': 0.15
            }
        }
    
    @abstractmethod
    def assess_quality(self, data: Dict[str, Any]) -> QualityMetrics:
        """
        Ana kalite deÄŸerlendirme metodu
        Alt sÄ±nÄ±flar bu metodu implement etmeli
        """
        pass
    
    def calculate_overall_score(self, component_scores: Dict[str, float]) -> float:
        """
        BileÅŸen skorlarÄ±ndan genel skoru hesapla
        
        Args:
            component_scores: BileÅŸen skorlarÄ±
            
        Returns:
            float: Genel kalite skoru (0-100)
        """
        weights = self.config['weights']
        overall_score = 0
        
        for component, score in component_scores.items():
            weight = weights.get(component, 0)
            overall_score += score * weight
            
        return min(100, max(0, overall_score))
    
    def determine_grade(self, overall_score: float) -> str:
        """
        Genel skordan harf notu belirle
        
        Args:
            overall_score: Genel kalite skoru
            
        Returns:
            str: Harf notu (A, B, C, D)
        """
        thresholds = self.config['quality_thresholds']
        
        if overall_score >= thresholds['excellent']:
            return 'A'
        elif overall_score >= thresholds['good']:
            return 'B'
        elif overall_score >= thresholds['fair']:
            return 'C'
        else:
            return 'D'
    
    def validate_minimum_requirements(self, data: Dict[str, Any]) -> Tuple[bool, List[str]]:
        """
        Minimum gereksinimleri kontrol et
        
        Args:
            data: Veri seti bilgileri
            
        Returns:
            Tuple[bool, List[str]]: (Gereksinimler karÅŸÄ±landÄ± mÄ±, Eksikler listesi)
        """
        requirements = self.config['minimum_requirements']
        issues = []
        
        # Minimum gÃ¶rÃ¼ntÃ¼ sayÄ±sÄ± kontrolÃ¼
        total_images = data.get('total_images', 0)
        if total_images < requirements['min_total_images']:
            issues.append(f"Toplam gÃ¶rÃ¼ntÃ¼ sayÄ±sÄ± yetersiz: {total_images} < {requirements['min_total_images']}")
        
        # SÄ±nÄ±f baÅŸÄ±na minimum gÃ¶rÃ¼ntÃ¼ kontrolÃ¼
        class_counts = data.get('class_counts', {})
        min_per_class = requirements['min_images_per_class']
        
        for class_name, count in class_counts.items():
            if count < min_per_class:
                issues.append(f"'{class_name}' sÄ±nÄ±fÄ± iÃ§in yetersiz Ã¶rnek: {count} < {min_per_class}")
        
        # Minimum Ã§Ã¶zÃ¼nÃ¼rlÃ¼k kontrolÃ¼
        avg_resolution = data.get('average_resolution', 0)
        if avg_resolution < requirements['min_resolution']:
            issues.append(f"Ortalama Ã§Ã¶zÃ¼nÃ¼rlÃ¼k dÃ¼ÅŸÃ¼k: {avg_resolution} < {requirements['min_resolution']}")
        
        # Class imbalance kontrolÃ¼
        class_imbalance = data.get('class_imbalance_ratio', 0)
        max_imbalance = requirements['max_class_imbalance']
        if class_imbalance > max_imbalance:
            issues.append(f"SÄ±nÄ±f dengesizliÄŸi yÃ¼ksek: {class_imbalance:.2f} > {max_imbalance}")
        
        return len(issues) == 0, issues
    
    def generate_recommendations(self, issues: List[str], metrics: QualityMetrics) -> List[str]:
        """
        Tespit edilen sorunlara gÃ¶re Ã¶neriler oluÅŸtur
        
        Args:
            issues: Tespit edilen sorunlar
            metrics: Kalite metrikleri
            
        Returns:
            List[str]: Ã–neriler listesi
        """
        recommendations = []
        
        # GÃ¶rÃ¼ntÃ¼ kalitesi Ã¶nerileri
        if metrics.image_quality_score < 70:
            recommendations.append("GÃ¶rÃ¼ntÃ¼ kalitesini artÄ±rmak iÃ§in dÃ¼ÅŸÃ¼k kaliteli gÃ¶rÃ¼ntÃ¼leri filtreleyin")
            recommendations.append("GÃ¶rÃ¼ntÃ¼ Ã¶n iÅŸleme teknikleri (denoising, sharpening) uygulayÄ±n")
        
        # Annotation kalitesi Ã¶nerileri
        if metrics.annotation_quality_score < 70:
            recommendations.append("Annotation kalitesini artÄ±rmak iÃ§in manuel review yapÄ±n")
            recommendations.append("TutarsÄ±z annotation'larÄ± dÃ¼zeltin")
        
        # SÄ±nÄ±f dengesizliÄŸi Ã¶nerileri
        if metrics.class_balance_score < 70:
            recommendations.append("Az temsil edilen sÄ±nÄ±flar iÃ§in daha fazla veri toplayÄ±n")
            recommendations.append("Data augmentation teknikleri kullanÄ±n")
        
        # Eksiklik Ã¶nerileri
        if metrics.completeness_score < 80:
            recommendations.append("Eksik annotation'larÄ± tamamlayÄ±n")
            recommendations.append("Veri seti tutarlÄ±lÄ±ÄŸÄ±nÄ± kontrol edin")
        
        # Ã‡eÅŸitlilik Ã¶nerileri
        if metrics.diversity_score < 70:
            recommendations.append("FarklÄ± senaryolardan daha fazla veri ekleyin")
            recommendations.append("GÃ¶rÃ¼ntÃ¼ Ã§eÅŸitliliÄŸini artÄ±rmak iÃ§in farklÄ± aÃ§Ä±lar/Ä±ÅŸÄ±k koÅŸullarÄ± kullanÄ±n")
        
        return recommendations
    
    def save_assessment(self, output_path: str, metrics: QualityMetrics) -> bool:
        """
        DeÄŸerlendirme sonuÃ§larÄ±nÄ± kaydet
        
        Args:
            output_path: Ã‡Ä±ktÄ± dosya yolu
            metrics: Kalite metrikleri
            
        Returns:
            bool: KayÄ±t baÅŸarÄ±lÄ± mÄ±
        """
        try:
            output_file = Path(output_path)
            output_file.parent.mkdir(parents=True, exist_ok=True)
            
            # JSON formatÄ±nda kaydet
            import json
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(metrics.to_dict(), f, indent=2, ensure_ascii=False)
                
            self.logger.info(f"DeÄŸerlendirme sonuÃ§larÄ± kaydedildi: {output_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"KayÄ±t hatasÄ±: {str(e)}")
            return False
    
    def load_assessment(self, input_path: str) -> Optional[QualityMetrics]:
        """
        KaydedilmiÅŸ deÄŸerlendirme sonuÃ§larÄ±nÄ± yÃ¼kle
        
        Args:
            input_path: GiriÅŸ dosya yolu
            
        Returns:
            Optional[QualityMetrics]: YÃ¼klenen metrikler
        """
        try:
            import json
            with open(input_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            # QualityMetrics nesnesine dÃ¶nÃ¼ÅŸtÃ¼r
            return QualityMetrics(**data)
            
        except Exception as e:
            self.logger.error(f"YÃ¼kleme hatasÄ±: {str(e)}")
            return None
    
    def compare_assessments(self, 
                          metrics1: QualityMetrics, 
                          metrics2: QualityMetrics) -> Dict[str, float]:
        """
        Ä°ki deÄŸerlendirmeyi karÅŸÄ±laÅŸtÄ±r
        
        Args:
            metrics1: Ä°lk metrikler
            metrics2: Ä°kinci metrikler
            
        Returns:
            Dict[str, float]: KarÅŸÄ±laÅŸtÄ±rma sonuÃ§larÄ±
        """
        comparison = {}
        
        # Ana skorlarÄ± karÅŸÄ±laÅŸtÄ±r
        comparison['overall_score_diff'] = metrics2.overall_score - metrics1.overall_score
        comparison['image_quality_diff'] = metrics2.image_quality_score - metrics1.image_quality_score
        comparison['annotation_quality_diff'] = metrics2.annotation_quality_score - metrics1.annotation_quality_score
        comparison['completeness_diff'] = metrics2.completeness_score - metrics1.completeness_score
        comparison['diversity_diff'] = metrics2.diversity_score - metrics1.diversity_score
        comparison['consistency_diff'] = metrics2.consistency_score - metrics1.consistency_score
        
        # SayÄ±sal deÄŸerleri karÅŸÄ±laÅŸtÄ±r
        comparison['images_diff'] = metrics2.total_images - metrics1.total_images
        comparison['annotations_diff'] = metrics2.total_annotations - metrics1.total_annotations
        comparison['classes_diff'] = metrics2.num_classes - metrics1.num_classes
        
        return comparison
    
    def print_summary(self, metrics: QualityMetrics) -> None:
        """
        DeÄŸerlendirme Ã¶zetini yazdÄ±r
        
        Args:
            metrics: Kalite metrikleri
        """
        print("\n" + "="*60)
        print("           DATASET QUALITY ASSESSMENT SUMMARY")
        print("="*60)
        
        print(f"\nğŸ¯ OVERALL GRADE: {metrics.dataset_grade}")
        print(f"ğŸ“Š OVERALL SCORE: {metrics.overall_score:.1f}/100")
        
        print(f"\nğŸ“ˆ COMPONENT SCORES:")
        print(f"   ğŸ–¼ï¸  Image Quality:     {metrics.image_quality_score:.1f}/100")
        print(f"   ğŸ·ï¸  Annotation Quality: {metrics.annotation_quality_score:.1f}/100")
        print(f"   âœ… Completeness:      {metrics.completeness_score:.1f}/100")
        print(f"   ğŸ¨ Diversity:         {metrics.diversity_score:.1f}/100")
        print(f"   ğŸ”„ Consistency:       {metrics.consistency_score:.1f}/100")
        
        print(f"\nğŸ“Š DATASET STATISTICS:")
        print(f"   ğŸ“ Total Images:      {metrics.total_images:,}")
        print(f"   ğŸ·ï¸  Total Annotations: {metrics.total_annotations:,}")
        print(f"   ğŸ“‚ Number of Classes: {metrics.num_classes}")
        
        if metrics.issues_found:
            print(f"\nâš ï¸  ISSUES FOUND ({len(metrics.issues_found)}):")
            for i, issue in enumerate(metrics.issues_found[:5], 1):
                print(f"   {i}. {issue}")
            if len(metrics.issues_found) > 5:
                print(f"   ... ve {len(metrics.issues_found) - 5} tane daha")
        
        if metrics.recommendations:
            print(f"\nğŸ’¡ TOP RECOMMENDATIONS ({len(metrics.recommendations)}):")
            for i, rec in enumerate(metrics.recommendations[:5], 1):
                print(f"   {i}. {rec}")
            if len(metrics.recommendations) > 5:
                print(f"   ... ve {len(metrics.recommendations) - 5} tane daha")
        
        print("\n" + "="*60)
