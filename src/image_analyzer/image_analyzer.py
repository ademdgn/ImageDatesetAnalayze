#!/usr/bin/env python3
"""
Ana GÃ¶rÃ¼ntÃ¼ Analiz ModÃ¼lÃ¼

Bu modÃ¼l tÃ¼m gÃ¶rÃ¼ntÃ¼ analiz bileÅŸenlerini birleÅŸtiren ana sÄ±nÄ±fÄ± iÃ§erir.
"""

from typing import Dict, List, Optional, Any, Callable
from pathlib import Path

# Alt modÃ¼lleri import et
from .base_analyzer import BaseImageAnalyzer
from .quality_metrics import QualityMetricsCalculator
from .anomaly_detector import AnomalyDetector
from .statistics_calculator import StatisticsCalculator

class ImageAnalyzer(BaseImageAnalyzer):
    """KapsamlÄ± gÃ¶rÃ¼ntÃ¼ analizi yapan ana sÄ±nÄ±f"""
    
    def __init__(self, config: Optional[Dict] = None):
        """
        ImageAnalyzer sÄ±nÄ±fÄ±nÄ± baÅŸlat
        
        Args:
            config: Analiz konfigÃ¼rasyonu
        """
        super().__init__(config)
        
        # Alt bileÅŸenleri baÅŸlat
        self.quality_calculator = QualityMetricsCalculator(config)
        self.anomaly_detector = AnomalyDetector(config)
        self.statistics_calculator = StatisticsCalculator(config)
        
        # SonuÃ§ depolama
        self.analysis_results = {}
        self.image_features = []
        
    def analyze_image_properties(self, image_path: str) -> Dict[str, Any]:
        """
        Tek bir gÃ¶rÃ¼ntÃ¼nÃ¼n tÃ¼m Ã¶zelliklerini analiz et
        
        Args:
            image_path: GÃ¶rÃ¼ntÃ¼ dosyasÄ± yolu
            
        Returns:
            GÃ¶rÃ¼ntÃ¼ Ã¶zellikleri sÃ¶zlÃ¼ÄŸÃ¼
        """
        try:
            # GÃ¶rÃ¼ntÃ¼yÃ¼ yÃ¼kle
            pil_image, cv_image = self.load_image(image_path)
            if pil_image is None or cv_image is None:
                return {
                    'file_path': str(image_path),
                    'error': 'GÃ¶rÃ¼ntÃ¼ yÃ¼klenemedi',
                    'analysis_failed': True
                }
            
            # Temel Ã¶zellikler
            properties = self.get_basic_properties(image_path, pil_image)
            
            # Kalite metrikleri
            quality_metrics = self.quality_calculator.calculate_all_metrics(pil_image, cv_image)
            
            # TÃ¼m Ã¶zellikleri birleÅŸtir
            properties.update(quality_metrics)
            
            return properties
            
        except Exception as e:
            return {
                'file_path': str(image_path),
                'error': str(e),
                'analysis_failed': True
            }
    
    def analyze_dataset_images(self, image_paths: List[str], 
                             progress_callback: Optional[Callable] = None) -> Dict[str, Any]:
        """
        TÃ¼m veri seti gÃ¶rÃ¼ntÃ¼lerini analiz et
        
        Args:
            image_paths: GÃ¶rÃ¼ntÃ¼ dosyasÄ± yollarÄ± listesi
            progress_callback: Ä°lerleme callback fonksiyonu
            
        Returns:
            KapsamlÄ± analiz sonuÃ§larÄ± sÃ¶zlÃ¼ÄŸÃ¼
        """
        print(f"ğŸ” {len(image_paths)} gÃ¶rÃ¼ntÃ¼ analiz ediliyor...")
        
        all_properties = []
        failed_images = []
        
        # Her gÃ¶rÃ¼ntÃ¼yÃ¼ analiz et
        for i, image_path in enumerate(image_paths):
            if progress_callback:
                progress_callback(i + 1, len(image_paths))
            else:
                self.progress_callback(i + 1, len(image_paths), "GÃ¶rÃ¼ntÃ¼ler analiz ediliyor")
            
            properties = self.analyze_image_properties(image_path)
            
            if properties.get('analysis_failed', False):
                failed_images.append(properties)
            else:
                all_properties.append(properties)
                # Feature vektÃ¶rÃ¼ oluÅŸtur
                feature_vector = self._extract_feature_vector(properties)
                self.image_features.append(feature_vector)
        
        print(f"\nâœ… {len(all_properties)} gÃ¶rÃ¼ntÃ¼ baÅŸarÄ±yla analiz edildi")
        if failed_images:
            print(f"âŒ {len(failed_images)} gÃ¶rÃ¼ntÃ¼ analiz edilemedi")
        
        # KapsamlÄ± analiz yap
        return self._perform_comprehensive_analysis(all_properties, failed_images)
    
    def _extract_feature_vector(self, properties: Dict) -> List[float]:
        """
        GÃ¶rÃ¼ntÃ¼ Ã¶zelliklerinden feature vektÃ¶rÃ¼ Ã§Ä±kar
        
        Args:
            properties: GÃ¶rÃ¼ntÃ¼ Ã¶zellikleri
            
        Returns:
            Feature vektÃ¶rÃ¼
        """
        return [
            properties.get('width', 0),
            properties.get('height', 0),
            properties.get('aspect_ratio', 0),
            properties.get('brightness_mean', 0),
            properties.get('contrast_score', 0),
            properties.get('blur_score', 0),
            properties.get('edge_density', 0),
            properties.get('color_diversity', 0),
            properties.get('red_mean', 0),
            properties.get('green_mean', 0),
            properties.get('blue_mean', 0),
            properties.get('saturation_mean', 0),
            properties.get('sharpness_score', 0),
            properties.get('noise_score', 0)
        ]
    
    def _perform_comprehensive_analysis(self, properties_list: List[Dict], 
                                      failed_images: List[Dict]) -> Dict[str, Any]:
        """
        KapsamlÄ± veri seti analizi yap
        
        Args:
            properties_list: BaÅŸarÄ±lÄ± analiz edilen gÃ¶rÃ¼ntÃ¼ Ã¶zellikleri
            failed_images: BaÅŸarÄ±sÄ±z analiz edilen gÃ¶rÃ¼ntÃ¼ler
            
        Returns:
            KapsamlÄ± analiz sonuÃ§larÄ±
        """
        print("ğŸ“Š Veri seti istatistikleri hesaplanÄ±yor...")
        
        # Temel istatistikler
        dataset_stats = self.statistics_calculator.calculate_dataset_statistics(properties_list)
        
        # Ã‡eÅŸitlilik metrikleri
        diversity_metrics = self.statistics_calculator.calculate_diversity_metrics(properties_list)
        
        # Kalite Ã¶zeti
        quality_summary = self.statistics_calculator.generate_quality_summary(properties_list)
        
        # Korelasyon analizi
        correlations = self.statistics_calculator.calculate_correlations(properties_list)
        
        print("ğŸ” Anomaliler tespit ediliyor...")
        
        # Anomali tespiti
        anomaly_results = self.anomaly_detector.detect_all_anomalies(
            properties_list, self.image_features
        )
        
        print("ğŸ’¡ Ã–neriler oluÅŸturuluyor...")
        
        # Ã–neriler
        recommendations = self.statistics_calculator.generate_recommendations(
            properties_list, anomaly_results, diversity_metrics
        )
        
        # SonuÃ§larÄ± birleÅŸtir
        results = {
            # Temel bilgiler
            'total_images': len(properties_list) + len(failed_images),
            'analyzed_images': len(properties_list),
            'failed_images': len(failed_images),
            'success_rate': len(properties_list) / (len(properties_list) + len(failed_images)) if (len(properties_list) + len(failed_images)) > 0 else 0,
            
            # DetaylÄ± analizler
            'dataset_statistics': dataset_stats,
            'quality_summary': quality_summary,
            'diversity_metrics': diversity_metrics,
            'correlations': correlations,
            'anomaly_results': anomaly_results,
            
            # Ham veriler
            'image_properties': properties_list,
            'failed_analyses': failed_images,
            'feature_vectors': self.image_features,
            
            # Ã–neriler ve Ã¶zet
            'recommendations': recommendations,
            'analysis_summary': self._generate_analysis_summary(
                properties_list, quality_summary, diversity_metrics, anomaly_results
            )
        }
        
        # SonuÃ§larÄ± depola
        self.analysis_results = results
        return results
    
    def _generate_analysis_summary(self, properties_list: List[Dict], 
                                 quality_summary: Dict, diversity_metrics: Dict,
                                 anomaly_results: Dict) -> Dict[str, Any]:
        """
        Analiz Ã¶zeti oluÅŸtur
        
        Args:
            properties_list: GÃ¶rÃ¼ntÃ¼ Ã¶zellikleri listesi
            quality_summary: Kalite Ã¶zeti
            diversity_metrics: Ã‡eÅŸitlilik metrikleri
            anomaly_results: Anomali sonuÃ§larÄ±
            
        Returns:
            Analiz Ã¶zeti sÃ¶zlÃ¼ÄŸÃ¼
        """
        total_images = len(properties_list)
        
        # Genel kalite deÄŸerlendirmesi
        avg_quality = quality_summary.get('statistics', {}).get('mean', 0)
        quality_grade = quality_summary.get('overall_grade', 'F')
        
        # Ã‡eÅŸitlilik skoru
        diversity_score = diversity_metrics.get('overall_diversity_score', 0)
        
        # Anomali oranÄ±
        anomaly_rate = anomaly_results.get('summary', {}).get('anomaly_rate', 0)
        
        # Genel skor hesapla (0-100)
        overall_score = self._calculate_overall_dataset_score(
            avg_quality, diversity_score, anomaly_rate
        )
        
        return {
            'total_images': total_images,
            'average_quality_score': round(avg_quality, 2),
            'quality_grade': quality_grade,
            'diversity_score': round(diversity_score, 3),
            'anomaly_rate': round(anomaly_rate, 2),
            'overall_dataset_score': round(overall_score, 2),
            'dataset_health': self._get_dataset_health_status(overall_score),
            'key_insights': self._generate_key_insights(
                properties_list, quality_summary, diversity_metrics, anomaly_results
            )
        }
    
    def _calculate_overall_dataset_score(self, quality_score: float, 
                                       diversity_score: float, 
                                       anomaly_rate: float) -> float:
        """
        Genel veri seti skoru hesapla
        
        Args:
            quality_score: Ortalama kalite skoru
            diversity_score: Ã‡eÅŸitlilik skoru
            anomaly_rate: Anomali oranÄ±
            
        Returns:
            Genel skor (0-100)
        """
        # SkorlarÄ± normalize et
        quality_norm = quality_score / 100.0
        diversity_norm = min(diversity_score, 1.0)
        anomaly_penalty = max(0, 1.0 - (anomaly_rate / 100.0))
        
        # AÄŸÄ±rlÄ±klÄ± ortalama
        weights = self.config.get('overall_score_weights', {
            'quality': 0.5,
            'diversity': 0.3,
            'anomaly_penalty': 0.2
        })
        
        overall_score = (
            quality_norm * weights.get('quality', 0.5) +
            diversity_norm * weights.get('diversity', 0.3) +
            anomaly_penalty * weights.get('anomaly_penalty', 0.2)
        ) * 100
        
        return overall_score
    
    def _get_dataset_health_status(self, overall_score: float) -> str:
        """
        Genel skora gÃ¶re veri seti saÄŸlÄ±k durumu
        
        Args:
            overall_score: Genel skor
            
        Returns:
            SaÄŸlÄ±k durumu string'i
        """
        if overall_score >= 85:
            return "MÃ¼kemmel"
        elif overall_score >= 75:
            return "Ä°yi"
        elif overall_score >= 65:
            return "Orta"
        elif overall_score >= 50:
            return "ZayÄ±f"
        else:
            return "Yetersiz"
    
    def _generate_key_insights(self, properties_list: List[Dict], 
                             quality_summary: Dict, diversity_metrics: Dict,
                             anomaly_results: Dict) -> List[str]:
        """
        Anahtar bulgular oluÅŸtur
        
        Args:
            properties_list: GÃ¶rÃ¼ntÃ¼ Ã¶zellikleri listesi
            quality_summary: Kalite Ã¶zeti
            diversity_metrics: Ã‡eÅŸitlilik metrikleri
            anomaly_results: Anomali sonuÃ§larÄ±
            
        Returns:
            Anahtar bulgular listesi
        """
        insights = []
        
        total_images = len(properties_list)
        
        # Veri boyutu deÄŸerlendirmesi
        if total_images < 100:
            insights.append(f"ğŸ“Š KÃ¼Ã§Ã¼k veri seti ({total_images} gÃ¶rÃ¼ntÃ¼) - Daha fazla veri gerekebilir")
        elif total_images < 1000:
            insights.append(f"ğŸ“Š Orta boyutlu veri seti ({total_images} gÃ¶rÃ¼ntÃ¼)")
        else:
            insights.append(f"ğŸ“Š BÃ¼yÃ¼k veri seti ({total_images} gÃ¶rÃ¼ntÃ¼)")
        
        # Kalite deÄŸerlendirmesi
        quality_dist = quality_summary.get('distribution', {})
        excellent_pct = quality_dist.get('excellent', {}).get('percentage', 0)
        poor_pct = quality_dist.get('poor', {}).get('percentage', 0)
        
        if excellent_pct > 50:
            insights.append(f"âœ¨ %{excellent_pct:.1f} mÃ¼kemmel kaliteli gÃ¶rÃ¼ntÃ¼")
        if poor_pct > 10:
            insights.append(f"âš ï¸ %{poor_pct:.1f} dÃ¼ÅŸÃ¼k kaliteli gÃ¶rÃ¼ntÃ¼")
        
        # Ã‡eÅŸitlilik deÄŸerlendirmesi
        resolution_diversity = diversity_metrics.get('resolution_diversity', 0)
        if resolution_diversity > 0.7:
            insights.append("ğŸŒˆ YÃ¼ksek Ã§Ã¶zÃ¼nÃ¼rlÃ¼k Ã§eÅŸitliliÄŸi")
        elif resolution_diversity < 0.3:
            insights.append("ğŸ“ DÃ¼ÅŸÃ¼k Ã§Ã¶zÃ¼nÃ¼rlÃ¼k Ã§eÅŸitliliÄŸi")
        
        # Anomali deÄŸerlendirmesi
        anomaly_summary = anomaly_results.get('summary', {})
        total_anomalies = anomaly_summary.get('total_anomalies', 0)
        if total_anomalies > 0:
            insights.append(f"ğŸ” {total_anomalies} anomali tespit edildi")
        
        # Format analizi
        if 'dataset_statistics' in self.analysis_results:
            format_dist = self.analysis_results['dataset_statistics'].get('format_distribution', {})
            unique_formats = format_dist.get('unique_formats', 0)
            if unique_formats > 3:
                insights.append(f"ğŸ”§ {unique_formats} farklÄ± format kullanÄ±lÄ±yor")
        
        return insights
    
    def get_analysis_summary_text(self) -> str:
        """
        Analiz Ã¶zetini metin olarak al
        
        Returns:
            Metin formatÄ±nda analiz Ã¶zeti
        """
        if not self.analysis_results:
            return "HenÃ¼z analiz yapÄ±lmadÄ±."
        
        summary = self.analysis_results.get('analysis_summary', {})
        
        text = f"""
ğŸ” VERÄ° SETÄ° ANALÄ°Z Ã–ZETÄ°
{'='*50}

ğŸ“Š Temel Bilgiler:
â€¢ Toplam gÃ¶rÃ¼ntÃ¼: {summary.get('total_images', 0)}
â€¢ Ortalama kalite: {summary.get('average_quality_score', 0)}/100 (Not: {summary.get('quality_grade', 'F')})
â€¢ Ã‡eÅŸitlilik skoru: {summary.get('diversity_score', 0):.3f}
â€¢ Anomali oranÄ±: %{summary.get('anomaly_rate', 0)}

ğŸ¯ Genel DeÄŸerlendirme:
â€¢ Veri seti skoru: {summary.get('overall_dataset_score', 0)}/100
â€¢ SaÄŸlÄ±k durumu: {summary.get('dataset_health', 'Bilinmiyor')}

ğŸ’¡ Anahtar Bulgular:
"""
        
        for insight in summary.get('key_insights', []):
            text += f"â€¢ {insight}\n"
        
        text += f"\nğŸ”§ Ã–neriler:\n"
        for recommendation in self.analysis_results.get('recommendations', []):
            text += f"â€¢ {recommendation}\n"
        
        return text
    
    def save_analysis_results(self, output_path: str) -> bool:
        """
        Analiz sonuÃ§larÄ±nÄ± JSON dosyasÄ±na kaydet
        
        Args:
            output_path: Ã‡Ä±ktÄ± dosyasÄ± yolu
            
        Returns:
            BaÅŸarÄ± durumu
        """
        try:
            import json
            
            # Ã‡Ä±ktÄ± dizinini oluÅŸtur
            Path(output_path).parent.mkdir(parents=True, exist_ok=True)
            
            # NumPy array'leri listlere dÃ¶nÃ¼ÅŸtÃ¼r
            results_copy = self._prepare_results_for_json(self.analysis_results)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results_copy, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… Analiz sonuÃ§larÄ± kaydedildi: {output_path}")
            return True
            
        except Exception as e:
            print(f"âŒ SonuÃ§lar kaydedilirken hata: {e}")
            return False
    
    def _prepare_results_for_json(self, data: Any) -> Any:
        """
        JSON serileÅŸtirme iÃ§in veriyi hazÄ±rla
        
        Args:
            data: SerileÅŸtirilecek veri
            
        Returns:
            JSON uyumlu veri
        """
        import numpy as np
        
        if isinstance(data, dict):
            return {key: self._prepare_results_for_json(value) for key, value in data.items()}
        elif isinstance(data, list):
            return [self._prepare_results_for_json(item) for item in data]
        elif isinstance(data, np.ndarray):
            return data.tolist()
        elif isinstance(data, (np.int64, np.int32)):
            return int(data)
        elif isinstance(data, (np.float64, np.float32)):
            return float(data)
        else:
            return data
    
    def quick_analyze_sample(self, image_paths: List[str], 
                           sample_size: int = 50) -> Dict[str, Any]:
        """
        HÄ±zlÄ± Ã¶rnek analizi yap
        
        Args:
            image_paths: TÃ¼m gÃ¶rÃ¼ntÃ¼ yollarÄ±
            sample_size: Analiz edilecek Ã¶rnek boyutu
            
        Returns:
            HÄ±zlÄ± analiz sonuÃ§larÄ±
        """
        import random
        
        # Rastgele Ã¶rnek seÃ§
        if len(image_paths) <= sample_size:
            sample_paths = image_paths
        else:
            sample_paths = random.sample(image_paths, sample_size)
        
        print(f"ğŸš€ HÄ±zlÄ± analiz: {len(sample_paths)} gÃ¶rÃ¼ntÃ¼ Ã¶rneÄŸi")
        
        # Analiz yap
        results = self.analyze_dataset_images(sample_paths)
        
        # Ã–rnek analizi olduÄŸunu belirt
        results['is_sample_analysis'] = True
        results['sample_size'] = len(sample_paths)
        results['total_available'] = len(image_paths)
        results['sample_ratio'] = len(sample_paths) / len(image_paths)
        
        return results
